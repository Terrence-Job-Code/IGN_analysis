{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just want to take notes on some ideas\n",
    "- look at ign top 100 (or some other number), see if I can come up with my own and compare to their list\n",
    "    - can try a few different models to see how they vary \n",
    "    - will clearly have a problem for games that don't have reviews\n",
    "- think of a way to visualize the semantic meaning (polarity and other thing) of words with the score\n",
    "    - plot it based on various authors, certain authors might just be more negative\n",
    "- compare various semantic meanings of authors\n",
    "- examine lengths of reviews based on score and author\n",
    "- maybe compare to other review website\n",
    "- try to create a feature to rank all games\n",
    "- could expand this to reviewers, and what I can find elsewhere\n",
    "- look at release dates of games (what day and time of year)\n",
    "    - might be a correlation between high ranked games and time of year/day\n",
    "- try to create a feature where you write a review, and then it assigns a score based on the review\n",
    "- try to find features that most impact scores\n",
    "\n",
    "\n",
    "- use various advanced deep learning NN to try to predict scores based on words\n",
    "- try gpt-2-simple (or something like it) to predict what a review would say\n",
    "- try other review generation methods to come up with reviews\n",
    "    - create your own review generation method\n",
    "- make a GAN to trick some of my classifiers about how real the review is\n",
    "    - if I can trick it, compare that review with real reviews, see if it's meaningful\n",
    "    \n",
    "- try to generate an IGN review based on information from other places (tags about game, comments from other websites, etc)\n",
    "\n",
    "- could try to use some aspects of the model (comparing score to review) to see if it works for movie reviews as well\n",
    "- Aside from the link, you could also try some unsupervised techniques to see what words or phrases show up the most, along with trying that with ngrams.\n",
    "- When I try to remove sections in the text (start div ... end div) I can create a second function to count how many remain at the end to get an idea of how clean the text is\n",
    "\n",
    "- can examine individual sentences and see which ones have the most positive sentiment\n",
    "- can try to get text from Game Grumps to see how they feel about given games\n",
    "- maybe try to find some examples of very polar language\n",
    "\n",
    "- Could also try training my own model to perform sentiment analysis, and see if that yields better results over textblob, and at the very least just compare with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some questions and ideas raised in my first eda pass\n",
    "\n",
    "In an effort to understand these plots a little better, lets take some averages of the data in bins (either half a point to a point) and see average word length. Should also plot the standard deviation\n",
    "\n",
    "- Could try to redistribute scores based on my understanding of their score criteria (https://corp.ign.com/review-practices) and just renormalizing it. \n",
    "\n",
    "- Could ask the question, what's the probability that a low score review gets many words written about it (bayesian stats BaBy!)\n",
    "\n",
    "- Can also compare the Bayesian approach to asking the question above and compare it to moving some number of standard deviations away from the mean\n",
    "\n",
    "- Questions: Are there similar tags between all games with short word counts? Are they mobile games? Or do a lot of them have authors in common?\n",
    "\n",
    "# Additional questions from the same file\n",
    "\n",
    "Here I've split up the score into bins (not of equal sizes) to see average word count for each bin. Some follow up questions:\n",
    "- How does the data look if I split the bins equally, i.e. if I have 10k reviews, each bin gets 1000 reviews going from lowest score to highest. What if I do 5 bins? This might give me a quick way to renormalize all of my data for a new scoring metric. How would this compare to mapping the scoring system to a normalized gaussian?\n",
    "\n",
    "- What if I create word bins? What is the average score for a given length of an article?\n",
    "\n",
    "- Should try to examine word length based on author, maybe some people just write more. And maybe some authors will specifically have a stronger polarity indicator\n",
    "\n",
    "- The initial results from the scoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Ideas\n",
    "- eliminate/partition reviews based on platform\n",
    "- eliminate reviews based on whether they are dlc or full games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premise for deep learning stuff\n",
    "\n",
    "Say you want to write reviews for a website or a blog, but you don't want to spend time writing reviews or playing the game. \n",
    "\n",
    "Can try and compare gpt2 based text generator model with a markov chain, and maybe some other text generator models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.175\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "my_statement = TextBlob('most games are not worth your time')\n",
    "print(my_statement.polarity)\n",
    "print(my_statement.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
