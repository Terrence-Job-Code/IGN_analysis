{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just want to take notes on some ideas\n",
    "- look at ign top 100 (or some other number), see if I can come up with my own and compare to their list\n",
    "    - can try a few different models to see how they vary \n",
    "    - will clearly have a problem for games that don't have reviews\n",
    "- think of a way to visualize the semantic meaning (polarity and other thing) of words with the score\n",
    "    - plot it based on various authors, certain authors might just be more negative\n",
    "- compare various semantic meanings of authors\n",
    "- examine lengths of reviews based on score and author\n",
    "- maybe compare to other review website\n",
    "- try to create a feature to rank all games\n",
    "- could expand this to reviewers, and what I can find elsewhere\n",
    "- look at release dates of games (what day and time of year)\n",
    "    - might be a correlation between high ranked games and time of year/day\n",
    "- try to create a feature where you write a review, and then it assigns a score based on the review\n",
    "- try to find features that most impact scores\n",
    "\n",
    "\n",
    "- use various advanced deep learning NN to try to predict scores based on words\n",
    "- try gpt-2-simple (or something like it) to predict what a review would say\n",
    "- try other review generation methods to come up with reviews\n",
    "    - create your own review generation method\n",
    "- make a GAN to trick some of my classifiers about how real the review is\n",
    "    - if I can trick it, compare that review with real reviews, see if it's meaningful\n",
    "    \n",
    "- try to generate an IGN review based on information from other places (tags about game, comments from other websites, etc)\n",
    "\n",
    "- could try to use some aspects of the model (comparing score to review) to see if it works for movie reviews as well\n",
    "- Aside from the link, you could also try some unsupervised techniques to see what words or phrases show up the most, along with trying that with ngrams.\n",
    "- When I try to remove sections in the text (start div ... end div) I can create a second function to count how many remain at the end to get an idea of how clean the text is\n",
    "\n",
    "- can examine individual sentences and see which ones have the most positive sentiment\n",
    "- can try to get text from Game Grumps to see how they feel about given games\n",
    "- maybe try to find some examples of very polar language\n",
    "\n",
    "- Could also try training my own model to perform sentiment analysis, and see if that yields better results over textblob, and at the very least just compare with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some questions and ideas raised in my first eda pass\n",
    "\n",
    "In an effort to understand these plots a little better, lets take some averages of the data in bins (either half a point to a point) and see average word length. Should also plot the standard deviation\n",
    "\n",
    "- Could try to redistribute scores based on my understanding of their score criteria (https://corp.ign.com/review-practices) and just renormalizing it. \n",
    "\n",
    "- Could ask the question, what's the probability that a low score review gets many words written about it (bayesian stats BaBy!)\n",
    "\n",
    "- Can also compare the Bayesian approach to asking the question above and compare it to moving some number of standard deviations away from the mean\n",
    "\n",
    "- Questions: Are there similar tags between all games with short word counts? Are they mobile games? Or do a lot of them have authors in common?\n",
    "\n",
    "# Additional questions from the same file\n",
    "\n",
    "Here I've split up the score into bins (not of equal sizes) to see average word count for each bin. Some follow up questions:\n",
    "- How does the data look if I split the bins equally, i.e. if I have 10k reviews, each bin gets 1000 reviews going from lowest score to highest. What if I do 5 bins? This might give me a quick way to renormalize all of my data for a new scoring metric. How would this compare to mapping the scoring system to a normalized gaussian?\n",
    "\n",
    "- What if I create word bins? What is the average score for a given length of an article?\n",
    "\n",
    "- Should try to examine word length based on author, maybe some people just write more. And maybe some authors will specifically have a stronger polarity indicator\n",
    "\n",
    "- The initial results from the scoring \n",
    "\n",
    "## Instead of looking at the polarity of the whole article, consider looking at the number of positive and negative words in an article (7/12/2020)\n",
    "- can even make cuts on word polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Ideas\n",
    "- eliminate/partition reviews based on platform\n",
    "- eliminate reviews based on whether they are dlc or full games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premise for deep learning stuff\n",
    "\n",
    "Say you want to write reviews for a website or a blog, but you don't want to spend time writing reviews or playing the game. \n",
    "\n",
    "Can try and compare gpt2 based text generator model with a markov chain, and maybe some other text generator models.\n",
    "\n",
    "### Primary goals for text generator and deep learning tools:\n",
    "1. Create a deep learning model that scores reviews\n",
    "    - Have various ways on trying to structure this. Currently, I have a thought that I could produce a score based on each sentence and average them, along with providing sentiment score and maybe some other info.\n",
    "    - If I come up with a different way, I can obviously try that as well\n",
    "    - Should compare results based on different data inputs\n",
    "2. ~Train GPT2 345M and generate texts, check to see how similar their results are to the base~\n",
    "3. Create a GAN network to see if GPT2 can fool a network\n",
    "    - This seems difficult, because I'm not exactly providing feedback to gpt2. I could try to produce an intermediary model that selects the text most likely to pass and train it. \n",
    "    - I could also train my own simple text generator and have it train against an identifier.\n",
    "        - This could be interesting, because it might be able to identify exactly what my DL network that's identifying real articles is looking for\n",
    " \n",
    "Could examine many of these things with LDA and try to find some kind of topics (games), and use to examine output of gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with GPT2\n",
    "- Could try to do a comparison of 355M model with training vs the untrained 774M model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 20, 2020 plans\n",
    "1. With a model that scores reviews, can use this to create a score for fake reviews\n",
    "2. With the fake review basically cemented, can create a network to try and tell real reviews from fake reviews\n",
    "3. Would also be neat to apply my previous analysis to the faked reviews as well\n",
    "\n",
    "Given that I have these things, but that basically involves training my models, I think it's time to start looking at fixing up some previous parts of my analysis\n",
    "\n",
    "Could also try to get a score from two models, along with a probability for that score, and then average scores that way\n",
    "\n",
    "# July 22, 2020\n",
    "\n",
    "### Some presentation notes:\n",
    "- Should make a table comparing results from different models. Have good ways of presenting my results \n",
    "- Start on the presentation by doing the analysis and results stuff first, then go back to the data cleaning and other less important parts of the presentation. My analysis sections are the meat of my presentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.175\n",
      "0.3\n",
      "Sentiment(polarity=0.175, subjectivity=0.3)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "my_statement = TextBlob('most games are not worth your time')\n",
    "print(my_statement.polarity)\n",
    "print(my_statement.subjectivity)\n",
    "print(my_statement.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\TerrenceJEdmonds\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.175\n",
      "0.3\n",
      "Sentiment(classification='pos', p_pos=0.6085411554070193, p_neg=0.39145884459298064)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('movie_reviews') # don't need to download a second time\n",
    "\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "my_statement_2 = TextBlob('most games are not worth your time', analyzer=NaiveBayesAnalyzer())\n",
    "print(my_statement_2.polarity)\n",
    "print(my_statement_2.subjectivity)\n",
    "print(my_statement_2.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this page is for notes, should make sure I understand what these results mean. Initially, I'm thinking these are probabilities, but on what distribution, and how should I use it going forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "word = 'help'\n",
    "word_test = TextBlob(word)\n",
    "print(word_test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
