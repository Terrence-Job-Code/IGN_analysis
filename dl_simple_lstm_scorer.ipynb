{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Scorer simple LSTM\n",
    "Going to create multiple files so I can run them separately with less concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import backend as k\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Conv1D, Dropout\n",
    "from keras.layers import MaxPooling1D, Flatten, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11072, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>rough_review_body</th>\n",
       "      <th>cleaned_review_body</th>\n",
       "      <th>review_summary_rough</th>\n",
       "      <th>review_score</th>\n",
       "      <th>tags</th>\n",
       "      <th>categories</th>\n",
       "      <th>modified_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>author_name</th>\n",
       "      <th>tag_list</th>\n",
       "      <th>categories_list</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_body_corpus</th>\n",
       "      <th>review_score_float</th>\n",
       "      <th>number_character_review</th>\n",
       "      <th>number_words_review</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.ign.com/articles/2011/07/19/warham...</td>\n",
       "      <td>\\u003csection class=\\\"article-page\\\"\\u003e\\u00...</td>\n",
       "      <td>'Warhammer 40K: Kill Team might be unfortun...</td>\n",
       "      <td>\"And \\\"also-ran\\\" hangs all over Warhammer 40K...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>:[\"review\",\"blogroll\",\"event-essentials\",\"hot\"...</td>\n",
       "      <td>:[\"ign\",\"ps3\",\"xbox-360\",\"xbox-live\"],</td>\n",
       "      <td>2011-10-18T05:59:32+0000</td>\n",
       "      <td>2011-07-19T19:47:00+0000</td>\n",
       "      <td>\"Arthur Gies\"</td>\n",
       "      <td>[review, blogroll, event-essentials, hot, lega...</td>\n",
       "      <td>[ign, ps3, xbox-360, xbox-live, ]</td>\n",
       "      <td>\"Warhammer 40K: Kill Team Review\"</td>\n",
       "      <td>warhammer 40k kill team might be unfortunat...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3705</td>\n",
       "      <td>642</td>\n",
       "      <td>0.062923</td>\n",
       "      <td>0.433371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.ign.com/articles/2011/07/20/call-o...</td>\n",
       "      <td>\\u003csection class=\\\"article-page\\\"\\u003eThe ...</td>\n",
       "      <td>The Call of Juarez franchise always intereste...</td>\n",
       "      <td>\"Call of Juarez: The Cartel is a poor change o...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>:[\"review\",\"blogroll\",\"call-of-juarez-the-cart...</td>\n",
       "      <td>:[\"ign\",\"pc\",\"ps3\",\"xbox-360\"],</td>\n",
       "      <td>2011-10-18T06:02:02+0000</td>\n",
       "      <td>2011-07-20T00:14:00+0000</td>\n",
       "      <td>\"Anthony Gallegos\"</td>\n",
       "      <td>[review, blogroll, call-of-juarez-the-cartel, ...</td>\n",
       "      <td>[ign, pc, ps3, xbox-360, ]</td>\n",
       "      <td>\"Call of Juarez: The Cartel Review\"</td>\n",
       "      <td>the call of juarez franchise always intereste...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3770</td>\n",
       "      <td>648</td>\n",
       "      <td>-0.016471</td>\n",
       "      <td>0.457898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ign.com/articles/2011/07/20/captai...</td>\n",
       "      <td>\\u003csection class=\\\"article-page\\\"\\u003eThe ...</td>\n",
       "      <td>The majority of movie games are just awful. S...</td>\n",
       "      <td>\"Captain America: Super Soldier is a mediocre ...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>:[\"review\",\"blogroll\",\"legacy\",\"wii\",\"games\",\"...</td>\n",
       "      <td>:[\"ign\",\"wii\"],</td>\n",
       "      <td>2011-10-18T06:02:20+0000</td>\n",
       "      <td>2011-07-20T00:43:00+0000</td>\n",
       "      <td>\"Audrey Drake\"</td>\n",
       "      <td>[review, blogroll, legacy, wii, games, captain...</td>\n",
       "      <td>[ign, wii, ]</td>\n",
       "      <td>\"Captain America: Super Soldier Wii Review\"</td>\n",
       "      <td>the majority of movie games are just awful st...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2675</td>\n",
       "      <td>474</td>\n",
       "      <td>-0.003842</td>\n",
       "      <td>0.544458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ign.com/articles/2011/07/20/quiz-c...</td>\n",
       "      <td>\\u003csection class=\\\"article-page\\\"\\u003e\\u00...</td>\n",
       "      <td>'Quiz Climber, from Buzz! developer   'Rele...</td>\n",
       "      <td>\"Ultimately, Quiz Climber is just a little too...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>:[\"review\",\"blogroll\",\"games\",\"legacy\",\"wirele...</td>\n",
       "      <td>:[\"ign\",\"wireless\"],</td>\n",
       "      <td>2011-10-18T06:07:06+0000</td>\n",
       "      <td>2011-07-20T21:44:00+0000</td>\n",
       "      <td>\"Justin Davis\"</td>\n",
       "      <td>[review, blogroll, games, legacy, wireless, qu...</td>\n",
       "      <td>[ign, wireless, ]</td>\n",
       "      <td>\"Quiz Climber Review\"</td>\n",
       "      <td>quiz climber from buzz developer   relentle...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1854</td>\n",
       "      <td>349</td>\n",
       "      <td>0.091160</td>\n",
       "      <td>0.600205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.ign.com/articles/2011/07/20/limbo-...</td>\n",
       "      <td>\\u003csection class=\\\"article-page\\\"\\u003eVide...</td>\n",
       "      <td>Video games are an art form made up of visual...</td>\n",
       "      <td>\"Limbo is an incredible achievement. Very few ...</td>\n",
       "      <td>9</td>\n",
       "      <td>:[\"review\",\"games\",\"legacy\",\"limbo\",\"platforme...</td>\n",
       "      <td>:[\"pc\",\"ps3\"],</td>\n",
       "      <td>2011-10-18T06:07:21+0000</td>\n",
       "      <td>2011-07-20T22:29:00+0000</td>\n",
       "      <td>\"Daemon Hatfield\"</td>\n",
       "      <td>[review, games, legacy, limbo, platformer, pla...</td>\n",
       "      <td>[pc, ps3, ]</td>\n",
       "      <td>\"Limbo Review\"</td>\n",
       "      <td>video games are an art form made up of visual...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2938</td>\n",
       "      <td>527</td>\n",
       "      <td>0.096551</td>\n",
       "      <td>0.525587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               links  \\\n",
       "1  https://www.ign.com/articles/2011/07/19/warham...   \n",
       "2  https://www.ign.com/articles/2011/07/20/call-o...   \n",
       "3  https://www.ign.com/articles/2011/07/20/captai...   \n",
       "4  https://www.ign.com/articles/2011/07/20/quiz-c...   \n",
       "5  https://www.ign.com/articles/2011/07/20/limbo-...   \n",
       "\n",
       "                                   rough_review_body  \\\n",
       "1  \\u003csection class=\\\"article-page\\\"\\u003e\\u00...   \n",
       "2  \\u003csection class=\\\"article-page\\\"\\u003eThe ...   \n",
       "3  \\u003csection class=\\\"article-page\\\"\\u003eThe ...   \n",
       "4  \\u003csection class=\\\"article-page\\\"\\u003e\\u00...   \n",
       "5  \\u003csection class=\\\"article-page\\\"\\u003eVide...   \n",
       "\n",
       "                                 cleaned_review_body  \\\n",
       "1     'Warhammer 40K: Kill Team might be unfortun...   \n",
       "2   The Call of Juarez franchise always intereste...   \n",
       "3   The majority of movie games are just awful. S...   \n",
       "4     'Quiz Climber, from Buzz! developer   'Rele...   \n",
       "5   Video games are an art form made up of visual...   \n",
       "\n",
       "                                review_summary_rough review_score  \\\n",
       "1  \"And \\\"also-ran\\\" hangs all over Warhammer 40K...          6.5   \n",
       "2  \"Call of Juarez: The Cartel is a poor change o...          4.5   \n",
       "3  \"Captain America: Super Soldier is a mediocre ...          5.5   \n",
       "4  \"Ultimately, Quiz Climber is just a little too...          6.5   \n",
       "5  \"Limbo is an incredible achievement. Very few ...            9   \n",
       "\n",
       "                                                tags  \\\n",
       "1  :[\"review\",\"blogroll\",\"event-essentials\",\"hot\"...   \n",
       "2  :[\"review\",\"blogroll\",\"call-of-juarez-the-cart...   \n",
       "3  :[\"review\",\"blogroll\",\"legacy\",\"wii\",\"games\",\"...   \n",
       "4  :[\"review\",\"blogroll\",\"games\",\"legacy\",\"wirele...   \n",
       "5  :[\"review\",\"games\",\"legacy\",\"limbo\",\"platforme...   \n",
       "\n",
       "                               categories             modified_date  \\\n",
       "1  :[\"ign\",\"ps3\",\"xbox-360\",\"xbox-live\"],  2011-10-18T05:59:32+0000   \n",
       "2         :[\"ign\",\"pc\",\"ps3\",\"xbox-360\"],  2011-10-18T06:02:02+0000   \n",
       "3                         :[\"ign\",\"wii\"],  2011-10-18T06:02:20+0000   \n",
       "4                    :[\"ign\",\"wireless\"],  2011-10-18T06:07:06+0000   \n",
       "5                          :[\"pc\",\"ps3\"],  2011-10-18T06:07:21+0000   \n",
       "\n",
       "             published_date         author_name  \\\n",
       "1  2011-07-19T19:47:00+0000       \"Arthur Gies\"   \n",
       "2  2011-07-20T00:14:00+0000  \"Anthony Gallegos\"   \n",
       "3  2011-07-20T00:43:00+0000      \"Audrey Drake\"   \n",
       "4  2011-07-20T21:44:00+0000      \"Justin Davis\"   \n",
       "5  2011-07-20T22:29:00+0000   \"Daemon Hatfield\"   \n",
       "\n",
       "                                            tag_list  \\\n",
       "1  [review, blogroll, event-essentials, hot, lega...   \n",
       "2  [review, blogroll, call-of-juarez-the-cartel, ...   \n",
       "3  [review, blogroll, legacy, wii, games, captain...   \n",
       "4  [review, blogroll, games, legacy, wireless, qu...   \n",
       "5  [review, games, legacy, limbo, platformer, pla...   \n",
       "\n",
       "                     categories_list  \\\n",
       "1  [ign, ps3, xbox-360, xbox-live, ]   \n",
       "2         [ign, pc, ps3, xbox-360, ]   \n",
       "3                       [ign, wii, ]   \n",
       "4                  [ign, wireless, ]   \n",
       "5                        [pc, ps3, ]   \n",
       "\n",
       "                                  review_title  \\\n",
       "1            \"Warhammer 40K: Kill Team Review\"   \n",
       "2          \"Call of Juarez: The Cartel Review\"   \n",
       "3  \"Captain America: Super Soldier Wii Review\"   \n",
       "4                        \"Quiz Climber Review\"   \n",
       "5                               \"Limbo Review\"   \n",
       "\n",
       "                                  review_body_corpus  review_score_float  \\\n",
       "1     warhammer 40k kill team might be unfortunat...                 6.5   \n",
       "2   the call of juarez franchise always intereste...                 4.5   \n",
       "3   the majority of movie games are just awful st...                 5.5   \n",
       "4     quiz climber from buzz developer   relentle...                 6.5   \n",
       "5   video games are an art form made up of visual...                 9.0   \n",
       "\n",
       "   number_character_review  number_words_review  polarity  subjectivity  \n",
       "1                     3705                  642  0.062923      0.433371  \n",
       "2                     3770                  648 -0.016471      0.457898  \n",
       "3                     2675                  474 -0.003842      0.544458  \n",
       "4                     1854                  349  0.091160      0.600205  \n",
       "5                     2938                  527  0.096551      0.525587  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ign_df = pd.read_pickle('ign_data/ign_data_pass_three.pkl')\n",
    "\n",
    "ign_df = ign_df.loc[ign_df['review_score']!='null']\n",
    "ign_df.reset_index(drop=True)\n",
    "\n",
    "print(ign_df.shape)\n",
    "ign_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min/max_df are document frequency\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n",
    "                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "ign_data = ign_df['cleaned_review_body'].values\n",
    "ign_scores = ign_df['review_score_float'].values \n",
    "\n",
    "data_train, data_validation, target_train, target_validation = train_test_split(ign_data, \n",
    "                                                                                ign_scores, \n",
    "                                                                                test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting training for ohe\n",
    "score_space = np.linspace(start=0.0,stop=10.0, num=101)\n",
    "temp_values = [[round(x, 2)] for x in score_space]\n",
    "# temp_values = [[str(round(x, 2))] for x in score_space]\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(temp_values)\n",
    "# print(enc.transform([[0.0], [0.3], [1.5]]).toarray())\n",
    "ign_score_format = [[round(x, 2)] for x in ign_scores]\n",
    "# ign_score_format = [[x] for x in ign_scores]\n",
    "ign_targets = enc.transform(ign_score_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8857, 101)\n",
      "(2215, 101)\n"
     ]
    }
   ],
   "source": [
    "# creating one hot encoding\n",
    "train_target_format = [[round(x, 2)] for x in target_train]\n",
    "valid_target_format = [[round(x, 2)] for x in target_validation]\n",
    "\n",
    "target_train_ohe = enc.transform(train_target_format)\n",
    "target_valid_ohe = enc.transform(valid_target_format)\n",
    "\n",
    "print(target_train_ohe.shape)\n",
    "print(target_valid_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n",
    "                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
    "data_train_cv = vectorizer.fit_transform(data_train)\n",
    "data_valid_cv = vectorizer.transform(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2208, 4430]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "preprocess = vectorizer.build_preprocessor()\n",
    "\n",
    "# print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    \n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    \n",
    "    return indexes\n",
    "\n",
    "print(to_sequence(tokenize, preprocess, word2idx, 'This is an important test!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 5000 5000 ... 2443 4030  332]\n"
     ]
    }
   ],
   "source": [
    "data_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in data_train]\n",
    "data_valid_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in data_validation]\n",
    "\n",
    "# padding sequences\n",
    "MAX_SEQ_LENGTH = len(max(data_train_sequences, key=len))\n",
    "if len(max(data_valid_sequences, key=len)) > MAX_SEQ_LENGTH:\n",
    "    MAX_SEQ_LENGTH = len(max(data_valid_sequences, key=len))\n",
    "    \n",
    "N_FEATURES = len(vectorizer.get_feature_names())\n",
    "\n",
    "data_train_sequences = pad_sequences(data_train_sequences, maxlen=MAX_SEQ_LENGTH, value=N_FEATURES)\n",
    "data_valid_sequences = pad_sequences(data_valid_sequences, maxlen=MAX_SEQ_LENGTH, value=N_FEATURES)\n",
    "\n",
    "print(data_train_sequences[0])\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(vectorizer.build_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2703, 64)          320064    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 101)               6565      \n",
      "=================================================================\n",
      "Total params: 359,653\n",
      "Trainable params: 359,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def simple_lstm_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(vectorizer.get_feature_names()) +1, 64, \n",
    "                        input_length=MAX_SEQ_LENGTH))\n",
    "    \n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(units=101, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "simple_lstm = simple_lstm_model()\n",
    "simple_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(simple_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8757 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "8757/8757 [==============================] - ETA: 40s - loss: 4.6152 - accuracy: 0.007 - ETA: 36s - loss: 4.6142 - accuracy: 0.011 - ETA: 32s - loss: 4.6133 - accuracy: 0.020 - ETA: 29s - loss: 4.6121 - accuracy: 0.024 - ETA: 27s - loss: 4.6110 - accuracy: 0.033 - ETA: 25s - loss: 4.6096 - accuracy: 0.040 - ETA: 22s - loss: 4.6081 - accuracy: 0.044 - ETA: 20s - loss: 4.6064 - accuracy: 0.049 - ETA: 18s - loss: 4.6046 - accuracy: 0.052 - ETA: 15s - loss: 4.6024 - accuracy: 0.055 - ETA: 13s - loss: 4.5997 - accuracy: 0.058 - ETA: 11s - loss: 4.5965 - accuracy: 0.060 - ETA: 9s - loss: 4.5926 - accuracy: 0.064 - ETA: 6s - loss: 4.5876 - accuracy: 0.06 - ETA: 4s - loss: 4.5810 - accuracy: 0.06 - ETA: 2s - loss: 4.5716 - accuracy: 0.06 - ETA: 0s - loss: 4.5555 - accuracy: 0.06 - 41s 5ms/step - loss: 4.5533 - accuracy: 0.0659 - val_loss: 4.0479 - val_accuracy: 0.1300\n",
      "Epoch 2/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 4.1371 - accuracy: 0.068 - ETA: 33s - loss: 4.0923 - accuracy: 0.072 - ETA: 31s - loss: 4.0643 - accuracy: 0.071 - ETA: 29s - loss: 4.0160 - accuracy: 0.073 - ETA: 26s - loss: 3.9932 - accuracy: 0.072 - ETA: 24s - loss: 3.9702 - accuracy: 0.072 - ETA: 22s - loss: 3.9427 - accuracy: 0.070 - ETA: 20s - loss: 3.9219 - accuracy: 0.069 - ETA: 17s - loss: 3.9146 - accuracy: 0.070 - ETA: 15s - loss: 3.8939 - accuracy: 0.073 - ETA: 13s - loss: 3.8779 - accuracy: 0.077 - ETA: 11s - loss: 3.8701 - accuracy: 0.079 - ETA: 9s - loss: 3.8602 - accuracy: 0.080 - ETA: 6s - loss: 3.8497 - accuracy: 0.08 - ETA: 4s - loss: 3.8398 - accuracy: 0.08 - ETA: 2s - loss: 3.8308 - accuracy: 0.08 - ETA: 0s - loss: 3.8265 - accuracy: 0.08 - 41s 5ms/step - loss: 3.8265 - accuracy: 0.0851 - val_loss: 3.5827 - val_accuracy: 0.0900\n",
      "Epoch 3/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.6410 - accuracy: 0.113 - ETA: 33s - loss: 3.6820 - accuracy: 0.097 - ETA: 31s - loss: 3.6843 - accuracy: 0.101 - ETA: 28s - loss: 3.6848 - accuracy: 0.099 - ETA: 26s - loss: 3.6819 - accuracy: 0.102 - ETA: 24s - loss: 3.6730 - accuracy: 0.100 - ETA: 22s - loss: 3.6766 - accuracy: 0.098 - ETA: 20s - loss: 3.6805 - accuracy: 0.096 - ETA: 17s - loss: 3.6848 - accuracy: 0.094 - ETA: 15s - loss: 3.6869 - accuracy: 0.095 - ETA: 13s - loss: 3.6864 - accuracy: 0.095 - ETA: 11s - loss: 3.6906 - accuracy: 0.092 - ETA: 9s - loss: 3.6966 - accuracy: 0.090 - ETA: 6s - loss: 3.6968 - accuracy: 0.09 - ETA: 4s - loss: 3.6940 - accuracy: 0.09 - ETA: 2s - loss: 3.6909 - accuracy: 0.08 - ETA: 0s - loss: 3.6842 - accuracy: 0.09 - 41s 5ms/step - loss: 3.6835 - accuracy: 0.0908 - val_loss: 3.5474 - val_accuracy: 0.0900\n",
      "Epoch 4/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.6374 - accuracy: 0.091 - ETA: 33s - loss: 3.6487 - accuracy: 0.098 - ETA: 31s - loss: 3.6505 - accuracy: 0.095 - ETA: 29s - loss: 3.6470 - accuracy: 0.095 - ETA: 26s - loss: 3.6630 - accuracy: 0.095 - ETA: 24s - loss: 3.6612 - accuracy: 0.095 - ETA: 22s - loss: 3.6705 - accuracy: 0.093 - ETA: 20s - loss: 3.6689 - accuracy: 0.093 - ETA: 18s - loss: 3.6689 - accuracy: 0.092 - ETA: 15s - loss: 3.6730 - accuracy: 0.090 - ETA: 13s - loss: 3.6724 - accuracy: 0.092 - ETA: 11s - loss: 3.6749 - accuracy: 0.092 - ETA: 9s - loss: 3.6742 - accuracy: 0.093 - ETA: 6s - loss: 3.6654 - accuracy: 0.09 - ETA: 4s - loss: 3.6676 - accuracy: 0.09 - ETA: 2s - loss: 3.6665 - accuracy: 0.09 - ETA: 0s - loss: 3.6637 - accuracy: 0.09 - 41s 5ms/step - loss: 3.6639 - accuracy: 0.0950 - val_loss: 3.5326 - val_accuracy: 0.0900\n",
      "Epoch 5/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.6197 - accuracy: 0.082 - ETA: 32s - loss: 3.6252 - accuracy: 0.092 - ETA: 30s - loss: 3.6165 - accuracy: 0.094 - ETA: 28s - loss: 3.6294 - accuracy: 0.094 - ETA: 25s - loss: 3.6368 - accuracy: 0.097 - ETA: 23s - loss: 3.6405 - accuracy: 0.101 - ETA: 21s - loss: 3.6488 - accuracy: 0.101 - ETA: 19s - loss: 3.6499 - accuracy: 0.101 - ETA: 17s - loss: 3.6477 - accuracy: 0.103 - ETA: 15s - loss: 3.6511 - accuracy: 0.101 - ETA: 12s - loss: 3.6477 - accuracy: 0.101 - ETA: 10s - loss: 3.6487 - accuracy: 0.101 - ETA: 8s - loss: 3.6489 - accuracy: 0.104 - ETA: 6s - loss: 3.6509 - accuracy: 0.10 - ETA: 4s - loss: 3.6541 - accuracy: 0.10 - ETA: 2s - loss: 3.6556 - accuracy: 0.10 - ETA: 0s - loss: 3.6588 - accuracy: 0.09 - 39s 4ms/step - loss: 3.6581 - accuracy: 0.0996 - val_loss: 3.5349 - val_accuracy: 0.0900\n",
      "Epoch 6/100\n",
      "8757/8757 [==============================] - ETA: 33s - loss: 3.5868 - accuracy: 0.119 - ETA: 31s - loss: 3.6421 - accuracy: 0.113 - ETA: 29s - loss: 3.6399 - accuracy: 0.104 - ETA: 27s - loss: 3.6451 - accuracy: 0.102 - ETA: 25s - loss: 3.6422 - accuracy: 0.102 - ETA: 23s - loss: 3.6526 - accuracy: 0.097 - ETA: 21s - loss: 3.6361 - accuracy: 0.099 - ETA: 19s - loss: 3.6433 - accuracy: 0.097 - ETA: 17s - loss: 3.6495 - accuracy: 0.097 - ETA: 14s - loss: 3.6449 - accuracy: 0.096 - ETA: 12s - loss: 3.6497 - accuracy: 0.097 - ETA: 10s - loss: 3.6500 - accuracy: 0.096 - ETA: 8s - loss: 3.6527 - accuracy: 0.095 - ETA: 6s - loss: 3.6552 - accuracy: 0.09 - ETA: 4s - loss: 3.6579 - accuracy: 0.09 - ETA: 2s - loss: 3.6571 - accuracy: 0.09 - ETA: 0s - loss: 3.6547 - accuracy: 0.09 - 39s 4ms/step - loss: 3.6542 - accuracy: 0.0959 - val_loss: 3.5274 - val_accuracy: 0.0900\n",
      "Epoch 7/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.6643 - accuracy: 0.134 - ETA: 32s - loss: 3.6757 - accuracy: 0.113 - ETA: 30s - loss: 3.6611 - accuracy: 0.109 - ETA: 28s - loss: 3.6412 - accuracy: 0.110 - ETA: 25s - loss: 3.6460 - accuracy: 0.106 - ETA: 23s - loss: 3.6462 - accuracy: 0.104 - ETA: 21s - loss: 3.6469 - accuracy: 0.101 - ETA: 19s - loss: 3.6471 - accuracy: 0.100 - ETA: 17s - loss: 3.6406 - accuracy: 0.099 - ETA: 15s - loss: 3.6364 - accuracy: 0.099 - ETA: 13s - loss: 3.6337 - accuracy: 0.099 - ETA: 10s - loss: 3.6404 - accuracy: 0.099 - ETA: 8s - loss: 3.6410 - accuracy: 0.099 - ETA: 6s - loss: 3.6446 - accuracy: 0.09 - ETA: 4s - loss: 3.6486 - accuracy: 0.09 - ETA: 2s - loss: 3.6521 - accuracy: 0.09 - ETA: 0s - loss: 3.6507 - accuracy: 0.09 - 39s 4ms/step - loss: 3.6497 - accuracy: 0.0970 - val_loss: 3.5185 - val_accuracy: 0.0900\n",
      "Epoch 8/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.6321 - accuracy: 0.113 - ETA: 31s - loss: 3.6446 - accuracy: 0.110 - ETA: 29s - loss: 3.6556 - accuracy: 0.104 - ETA: 27s - loss: 3.6788 - accuracy: 0.098 - ETA: 25s - loss: 3.6713 - accuracy: 0.100 - ETA: 23s - loss: 3.6626 - accuracy: 0.099 - ETA: 21s - loss: 3.6602 - accuracy: 0.098 - ETA: 19s - loss: 3.6592 - accuracy: 0.098 - ETA: 17s - loss: 3.6619 - accuracy: 0.095 - ETA: 15s - loss: 3.6532 - accuracy: 0.096 - ETA: 12s - loss: 3.6546 - accuracy: 0.094 - ETA: 10s - loss: 3.6541 - accuracy: 0.096 - ETA: 8s - loss: 3.6556 - accuracy: 0.095 - ETA: 6s - loss: 3.6586 - accuracy: 0.09 - ETA: 4s - loss: 3.6509 - accuracy: 0.09 - ETA: 2s - loss: 3.6443 - accuracy: 0.09 - ETA: 0s - loss: 3.6439 - accuracy: 0.09 - 40s 5ms/step - loss: 3.6432 - accuracy: 0.0998 - val_loss: 3.5280 - val_accuracy: 0.0900\n",
      "Epoch 9/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.7277 - accuracy: 0.113 - ETA: 33s - loss: 3.6730 - accuracy: 0.109 - ETA: 31s - loss: 3.6760 - accuracy: 0.106 - ETA: 29s - loss: 3.6663 - accuracy: 0.109 - ETA: 27s - loss: 3.6517 - accuracy: 0.109 - ETA: 25s - loss: 3.6491 - accuracy: 0.104 - ETA: 22s - loss: 3.6491 - accuracy: 0.102 - ETA: 20s - loss: 3.6430 - accuracy: 0.104 - ETA: 18s - loss: 3.6320 - accuracy: 0.105 - ETA: 16s - loss: 3.6356 - accuracy: 0.104 - ETA: 13s - loss: 3.6345 - accuracy: 0.104 - ETA: 11s - loss: 3.6364 - accuracy: 0.104 - ETA: 9s - loss: 3.6330 - accuracy: 0.105 - ETA: 7s - loss: 3.6266 - accuracy: 0.10 - ETA: 4s - loss: 3.6298 - accuracy: 0.10 - ETA: 2s - loss: 3.6321 - accuracy: 0.10 - ETA: 0s - loss: 3.6342 - accuracy: 0.10 - 43s 5ms/step - loss: 3.6337 - accuracy: 0.1001 - val_loss: 3.5266 - val_accuracy: 0.0900\n",
      "Epoch 10/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.6587 - accuracy: 0.087 - ETA: 32s - loss: 3.6498 - accuracy: 0.088 - ETA: 30s - loss: 3.6325 - accuracy: 0.099 - ETA: 28s - loss: 3.6390 - accuracy: 0.103 - ETA: 26s - loss: 3.6426 - accuracy: 0.105 - ETA: 23s - loss: 3.6425 - accuracy: 0.105 - ETA: 21s - loss: 3.6381 - accuracy: 0.107 - ETA: 19s - loss: 3.6448 - accuracy: 0.106 - ETA: 17s - loss: 3.6463 - accuracy: 0.107 - ETA: 15s - loss: 3.6469 - accuracy: 0.106 - ETA: 13s - loss: 3.6374 - accuracy: 0.107 - ETA: 11s - loss: 3.6243 - accuracy: 0.108 - ETA: 8s - loss: 3.6303 - accuracy: 0.108 - ETA: 6s - loss: 3.6235 - accuracy: 0.10 - ETA: 4s - loss: 3.6242 - accuracy: 0.10 - ETA: 2s - loss: 3.6191 - accuracy: 0.10 - ETA: 0s - loss: 3.6209 - accuracy: 0.10 - 40s 5ms/step - loss: 3.6214 - accuracy: 0.1095 - val_loss: 3.5231 - val_accuracy: 0.1100\n",
      "Epoch 11/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.5639 - accuracy: 0.132 - ETA: 34s - loss: 3.5447 - accuracy: 0.136 - ETA: 32s - loss: 3.5662 - accuracy: 0.132 - ETA: 29s - loss: 3.5766 - accuracy: 0.141 - ETA: 27s - loss: 3.5883 - accuracy: 0.143 - ETA: 25s - loss: 3.6003 - accuracy: 0.141 - ETA: 22s - loss: 3.5968 - accuracy: 0.144 - ETA: 20s - loss: 3.5980 - accuracy: 0.146 - ETA: 18s - loss: 3.5970 - accuracy: 0.145 - ETA: 16s - loss: 3.6018 - accuracy: 0.144 - ETA: 13s - loss: 3.6063 - accuracy: 0.142 - ETA: 11s - loss: 3.6019 - accuracy: 0.142 - ETA: 9s - loss: 3.6029 - accuracy: 0.139 - ETA: 7s - loss: 3.6008 - accuracy: 0.13 - ETA: 4s - loss: 3.6014 - accuracy: 0.13 - ETA: 2s - loss: 3.6035 - accuracy: 0.13 - ETA: 0s - loss: 3.6041 - accuracy: 0.13 - 42s 5ms/step - loss: 3.6050 - accuracy: 0.1386 - val_loss: 3.5296 - val_accuracy: 0.1100\n",
      "Epoch 12/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.5979 - accuracy: 0.134 - ETA: 33s - loss: 3.5873 - accuracy: 0.129 - ETA: 31s - loss: 3.5770 - accuracy: 0.136 - ETA: 29s - loss: 3.5930 - accuracy: 0.135 - ETA: 26s - loss: 3.5923 - accuracy: 0.136 - ETA: 24s - loss: 3.5861 - accuracy: 0.140 - ETA: 22s - loss: 3.5887 - accuracy: 0.142 - ETA: 20s - loss: 3.5864 - accuracy: 0.147 - ETA: 18s - loss: 3.5917 - accuracy: 0.143 - ETA: 15s - loss: 3.5807 - accuracy: 0.144 - ETA: 13s - loss: 3.5783 - accuracy: 0.144 - ETA: 11s - loss: 3.5876 - accuracy: 0.142 - ETA: 9s - loss: 3.5856 - accuracy: 0.143 - ETA: 6s - loss: 3.5852 - accuracy: 0.14 - ETA: 4s - loss: 3.5840 - accuracy: 0.14 - ETA: 2s - loss: 3.5836 - accuracy: 0.14 - ETA: 0s - loss: 3.5808 - accuracy: 0.14 - 41s 5ms/step - loss: 3.5818 - accuracy: 0.1446 - val_loss: 3.5200 - val_accuracy: 0.1300\n",
      "Epoch 13/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.5672 - accuracy: 0.132 - ETA: 33s - loss: 3.6046 - accuracy: 0.145 - ETA: 31s - loss: 3.5978 - accuracy: 0.145 - ETA: 29s - loss: 3.5834 - accuracy: 0.147 - ETA: 26s - loss: 3.5715 - accuracy: 0.146 - ETA: 24s - loss: 3.5666 - accuracy: 0.145 - ETA: 22s - loss: 3.5720 - accuracy: 0.144 - ETA: 20s - loss: 3.5736 - accuracy: 0.144 - ETA: 17s - loss: 3.5724 - accuracy: 0.147 - ETA: 15s - loss: 3.5568 - accuracy: 0.148 - ETA: 13s - loss: 3.5550 - accuracy: 0.149 - ETA: 11s - loss: 3.5534 - accuracy: 0.148 - ETA: 9s - loss: 3.5532 - accuracy: 0.147 - ETA: 6s - loss: 3.5580 - accuracy: 0.14 - ETA: 4s - loss: 3.5570 - accuracy: 0.14 - ETA: 2s - loss: 3.5565 - accuracy: 0.14 - ETA: 0s - loss: 3.5539 - accuracy: 0.14 - 41s 5ms/step - loss: 3.5534 - accuracy: 0.1462 - val_loss: 3.5158 - val_accuracy: 0.1200\n",
      "Epoch 14/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.5763 - accuracy: 0.138 - ETA: 33s - loss: 3.5677 - accuracy: 0.141 - ETA: 31s - loss: 3.5671 - accuracy: 0.145 - ETA: 29s - loss: 3.5571 - accuracy: 0.149 - ETA: 26s - loss: 3.5519 - accuracy: 0.146 - ETA: 24s - loss: 3.5373 - accuracy: 0.153 - ETA: 22s - loss: 3.5268 - accuracy: 0.153 - ETA: 20s - loss: 3.5221 - accuracy: 0.152 - ETA: 17s - loss: 3.5268 - accuracy: 0.152 - ETA: 15s - loss: 3.5257 - accuracy: 0.154 - ETA: 13s - loss: 3.5251 - accuracy: 0.158 - ETA: 11s - loss: 3.5186 - accuracy: 0.162 - ETA: 9s - loss: 3.5142 - accuracy: 0.163 - ETA: 6s - loss: 3.5231 - accuracy: 0.16 - ETA: 4s - loss: 3.5231 - accuracy: 0.16 - ETA: 2s - loss: 3.5222 - accuracy: 0.16 - ETA: 0s - loss: 3.5200 - accuracy: 0.16 - 41s 5ms/step - loss: 3.5200 - accuracy: 0.1602 - val_loss: 3.5171 - val_accuracy: 0.1100\n",
      "Epoch 15/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.5217 - accuracy: 0.154 - ETA: 32s - loss: 3.4900 - accuracy: 0.175 - ETA: 30s - loss: 3.4881 - accuracy: 0.164 - ETA: 28s - loss: 3.4919 - accuracy: 0.170 - ETA: 25s - loss: 3.4870 - accuracy: 0.172 - ETA: 23s - loss: 3.4758 - accuracy: 0.177 - ETA: 21s - loss: 3.4763 - accuracy: 0.176 - ETA: 19s - loss: 3.4789 - accuracy: 0.174 - ETA: 17s - loss: 3.4850 - accuracy: 0.173 - ETA: 15s - loss: 3.4932 - accuracy: 0.172 - ETA: 13s - loss: 3.4880 - accuracy: 0.171 - ETA: 10s - loss: 3.4892 - accuracy: 0.173 - ETA: 8s - loss: 3.4863 - accuracy: 0.172 - ETA: 6s - loss: 3.4834 - accuracy: 0.17 - ETA: 4s - loss: 3.4844 - accuracy: 0.17 - ETA: 2s - loss: 3.4824 - accuracy: 0.17 - ETA: 0s - loss: 3.4823 - accuracy: 0.17 - 40s 5ms/step - loss: 3.4814 - accuracy: 0.1767 - val_loss: 3.5155 - val_accuracy: 0.1300\n",
      "Epoch 16/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.4642 - accuracy: 0.189 - ETA: 33s - loss: 3.4521 - accuracy: 0.192 - ETA: 31s - loss: 3.4579 - accuracy: 0.204 - ETA: 29s - loss: 3.4604 - accuracy: 0.204 - ETA: 27s - loss: 3.4569 - accuracy: 0.203 - ETA: 24s - loss: 3.4484 - accuracy: 0.207 - ETA: 22s - loss: 3.4512 - accuracy: 0.207 - ETA: 20s - loss: 3.4468 - accuracy: 0.208 - ETA: 18s - loss: 3.4507 - accuracy: 0.205 - ETA: 15s - loss: 3.4441 - accuracy: 0.203 - ETA: 13s - loss: 3.4392 - accuracy: 0.203 - ETA: 11s - loss: 3.4460 - accuracy: 0.203 - ETA: 9s - loss: 3.4465 - accuracy: 0.204 - ETA: 6s - loss: 3.4460 - accuracy: 0.20 - ETA: 4s - loss: 3.4435 - accuracy: 0.20 - ETA: 2s - loss: 3.4381 - accuracy: 0.20 - ETA: 0s - loss: 3.4400 - accuracy: 0.20 - 41s 5ms/step - loss: 3.4397 - accuracy: 0.2011 - val_loss: 3.5128 - val_accuracy: 0.1300\n",
      "Epoch 17/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.3761 - accuracy: 0.212 - ETA: 33s - loss: 3.4027 - accuracy: 0.211 - ETA: 31s - loss: 3.3992 - accuracy: 0.208 - ETA: 29s - loss: 3.4177 - accuracy: 0.199 - ETA: 26s - loss: 3.4207 - accuracy: 0.201 - ETA: 24s - loss: 3.4092 - accuracy: 0.204 - ETA: 22s - loss: 3.4242 - accuracy: 0.206 - ETA: 20s - loss: 3.4069 - accuracy: 0.212 - ETA: 17s - loss: 3.4075 - accuracy: 0.212 - ETA: 15s - loss: 3.4023 - accuracy: 0.213 - ETA: 13s - loss: 3.3942 - accuracy: 0.213 - ETA: 11s - loss: 3.3985 - accuracy: 0.211 - ETA: 9s - loss: 3.4001 - accuracy: 0.212 - ETA: 6s - loss: 3.3929 - accuracy: 0.21 - ETA: 4s - loss: 3.3929 - accuracy: 0.21 - ETA: 2s - loss: 3.3948 - accuracy: 0.21 - ETA: 0s - loss: 3.3941 - accuracy: 0.21 - 41s 5ms/step - loss: 3.3940 - accuracy: 0.2166 - val_loss: 3.5292 - val_accuracy: 0.1200\n",
      "Epoch 18/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.2743 - accuracy: 0.259 - ETA: 33s - loss: 3.3140 - accuracy: 0.227 - ETA: 30s - loss: 3.2922 - accuracy: 0.236 - ETA: 28s - loss: 3.2987 - accuracy: 0.235 - ETA: 26s - loss: 3.3164 - accuracy: 0.228 - ETA: 24s - loss: 3.3060 - accuracy: 0.235 - ETA: 22s - loss: 3.3097 - accuracy: 0.236 - ETA: 20s - loss: 3.3205 - accuracy: 0.232 - ETA: 18s - loss: 3.3286 - accuracy: 0.229 - ETA: 15s - loss: 3.3313 - accuracy: 0.228 - ETA: 13s - loss: 3.3331 - accuracy: 0.227 - ETA: 11s - loss: 3.3377 - accuracy: 0.226 - ETA: 9s - loss: 3.3377 - accuracy: 0.227 - ETA: 6s - loss: 3.3382 - accuracy: 0.22 - ETA: 4s - loss: 3.3364 - accuracy: 0.22 - ETA: 2s - loss: 3.3401 - accuracy: 0.22 - ETA: 0s - loss: 3.3440 - accuracy: 0.22 - 41s 5ms/step - loss: 3.3442 - accuracy: 0.2251 - val_loss: 3.5385 - val_accuracy: 0.1100\n",
      "Epoch 19/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.3190 - accuracy: 0.226 - ETA: 33s - loss: 3.2614 - accuracy: 0.241 - ETA: 31s - loss: 3.2911 - accuracy: 0.239 - ETA: 29s - loss: 3.2894 - accuracy: 0.234 - ETA: 27s - loss: 3.2754 - accuracy: 0.240 - ETA: 24s - loss: 3.2886 - accuracy: 0.239 - ETA: 22s - loss: 3.3074 - accuracy: 0.236 - ETA: 20s - loss: 3.3040 - accuracy: 0.235 - ETA: 18s - loss: 3.3042 - accuracy: 0.233 - ETA: 15s - loss: 3.2997 - accuracy: 0.236 - ETA: 13s - loss: 3.2950 - accuracy: 0.239 - ETA: 11s - loss: 3.2877 - accuracy: 0.239 - ETA: 9s - loss: 3.2870 - accuracy: 0.240 - ETA: 6s - loss: 3.2843 - accuracy: 0.24 - ETA: 4s - loss: 3.2872 - accuracy: 0.24 - ETA: 2s - loss: 3.2872 - accuracy: 0.24 - ETA: 0s - loss: 3.2915 - accuracy: 0.23 - 41s 5ms/step - loss: 3.2919 - accuracy: 0.2392 - val_loss: 3.5429 - val_accuracy: 0.0900\n",
      "Epoch 20/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.0764 - accuracy: 0.275 - ETA: 33s - loss: 3.2265 - accuracy: 0.250 - ETA: 30s - loss: 3.2716 - accuracy: 0.248 - ETA: 28s - loss: 3.2694 - accuracy: 0.249 - ETA: 26s - loss: 3.2691 - accuracy: 0.248 - ETA: 24s - loss: 3.2604 - accuracy: 0.251 - ETA: 22s - loss: 3.2607 - accuracy: 0.250 - ETA: 20s - loss: 3.2469 - accuracy: 0.254 - ETA: 18s - loss: 3.2422 - accuracy: 0.257 - ETA: 16s - loss: 3.2439 - accuracy: 0.257 - ETA: 13s - loss: 3.2425 - accuracy: 0.258 - ETA: 11s - loss: 3.2455 - accuracy: 0.258 - ETA: 9s - loss: 3.2474 - accuracy: 0.258 - ETA: 7s - loss: 3.2521 - accuracy: 0.25 - ETA: 4s - loss: 3.2468 - accuracy: 0.25 - ETA: 2s - loss: 3.2406 - accuracy: 0.25 - ETA: 0s - loss: 3.2401 - accuracy: 0.25 - 41s 5ms/step - loss: 3.2403 - accuracy: 0.2587 - val_loss: 3.5641 - val_accuracy: 0.1400\n",
      "Epoch 21/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.2803 - accuracy: 0.242 - ETA: 32s - loss: 3.2250 - accuracy: 0.261 - ETA: 30s - loss: 3.2332 - accuracy: 0.257 - ETA: 28s - loss: 3.2018 - accuracy: 0.263 - ETA: 26s - loss: 3.2019 - accuracy: 0.263 - ETA: 23s - loss: 3.1830 - accuracy: 0.266 - ETA: 21s - loss: 3.1866 - accuracy: 0.263 - ETA: 19s - loss: 3.1762 - accuracy: 0.267 - ETA: 17s - loss: 3.1894 - accuracy: 0.265 - ETA: 15s - loss: 3.1858 - accuracy: 0.266 - ETA: 13s - loss: 3.1862 - accuracy: 0.265 - ETA: 10s - loss: 3.1906 - accuracy: 0.265 - ETA: 8s - loss: 3.1975 - accuracy: 0.262 - ETA: 6s - loss: 3.1969 - accuracy: 0.26 - ETA: 4s - loss: 3.1963 - accuracy: 0.26 - ETA: 2s - loss: 3.1951 - accuracy: 0.26 - ETA: 0s - loss: 3.1869 - accuracy: 0.26 - 40s 5ms/step - loss: 3.1866 - accuracy: 0.2642 - val_loss: 3.5827 - val_accuracy: 0.1400\n",
      "Epoch 22/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 3.1638 - accuracy: 0.287 - ETA: 32s - loss: 3.1374 - accuracy: 0.272 - ETA: 30s - loss: 3.1215 - accuracy: 0.279 - ETA: 28s - loss: 3.1381 - accuracy: 0.278 - ETA: 25s - loss: 3.1388 - accuracy: 0.281 - ETA: 23s - loss: 3.1420 - accuracy: 0.282 - ETA: 21s - loss: 3.1384 - accuracy: 0.281 - ETA: 19s - loss: 3.1491 - accuracy: 0.279 - ETA: 17s - loss: 3.1504 - accuracy: 0.277 - ETA: 15s - loss: 3.1354 - accuracy: 0.279 - ETA: 13s - loss: 3.1412 - accuracy: 0.277 - ETA: 10s - loss: 3.1424 - accuracy: 0.279 - ETA: 8s - loss: 3.1305 - accuracy: 0.279 - ETA: 6s - loss: 3.1318 - accuracy: 0.27 - ETA: 4s - loss: 3.1336 - accuracy: 0.27 - ETA: 2s - loss: 3.1328 - accuracy: 0.27 - ETA: 0s - loss: 3.1333 - accuracy: 0.27 - 40s 5ms/step - loss: 3.1346 - accuracy: 0.2767 - val_loss: 3.5816 - val_accuracy: 0.1200\n",
      "Epoch 23/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 3.0563 - accuracy: 0.298 - ETA: 32s - loss: 3.0966 - accuracy: 0.293 - ETA: 30s - loss: 3.0927 - accuracy: 0.284 - ETA: 27s - loss: 3.1099 - accuracy: 0.279 - ETA: 25s - loss: 3.1175 - accuracy: 0.280 - ETA: 23s - loss: 3.0993 - accuracy: 0.282 - ETA: 21s - loss: 3.1000 - accuracy: 0.282 - ETA: 19s - loss: 3.0934 - accuracy: 0.287 - ETA: 17s - loss: 3.0794 - accuracy: 0.292 - ETA: 15s - loss: 3.0831 - accuracy: 0.290 - ETA: 13s - loss: 3.0803 - accuracy: 0.289 - ETA: 10s - loss: 3.0743 - accuracy: 0.290 - ETA: 8s - loss: 3.0643 - accuracy: 0.295 - ETA: 6s - loss: 3.0672 - accuracy: 0.29 - ETA: 4s - loss: 3.0750 - accuracy: 0.29 - ETA: 2s - loss: 3.0755 - accuracy: 0.29 - ETA: 0s - loss: 3.0820 - accuracy: 0.28 - 40s 5ms/step - loss: 3.0827 - accuracy: 0.2891 - val_loss: 3.5955 - val_accuracy: 0.1400\n",
      "Epoch 24/100\n",
      "8757/8757 [==============================] - ETA: 40s - loss: 3.0026 - accuracy: 0.296 - ETA: 34s - loss: 3.0242 - accuracy: 0.294 - ETA: 32s - loss: 3.0561 - accuracy: 0.287 - ETA: 29s - loss: 3.0544 - accuracy: 0.287 - ETA: 27s - loss: 3.0368 - accuracy: 0.295 - ETA: 25s - loss: 3.0473 - accuracy: 0.287 - ETA: 23s - loss: 3.0541 - accuracy: 0.284 - ETA: 20s - loss: 3.0515 - accuracy: 0.284 - ETA: 18s - loss: 3.0553 - accuracy: 0.285 - ETA: 16s - loss: 3.0439 - accuracy: 0.289 - ETA: 13s - loss: 3.0374 - accuracy: 0.293 - ETA: 11s - loss: 3.0411 - accuracy: 0.292 - ETA: 9s - loss: 3.0452 - accuracy: 0.291 - ETA: 7s - loss: 3.0444 - accuracy: 0.29 - ETA: 4s - loss: 3.0486 - accuracy: 0.28 - ETA: 2s - loss: 3.0403 - accuracy: 0.29 - ETA: 0s - loss: 3.0352 - accuracy: 0.29 - 41s 5ms/step - loss: 3.0343 - accuracy: 0.2935 - val_loss: 3.6099 - val_accuracy: 0.1400\n",
      "Epoch 25/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.9889 - accuracy: 0.293 - ETA: 32s - loss: 2.9850 - accuracy: 0.286 - ETA: 30s - loss: 2.9773 - accuracy: 0.295 - ETA: 28s - loss: 2.9818 - accuracy: 0.299 - ETA: 26s - loss: 2.9671 - accuracy: 0.303 - ETA: 24s - loss: 2.9654 - accuracy: 0.301 - ETA: 21s - loss: 2.9694 - accuracy: 0.302 - ETA: 19s - loss: 2.9691 - accuracy: 0.302 - ETA: 17s - loss: 2.9766 - accuracy: 0.299 - ETA: 15s - loss: 2.9841 - accuracy: 0.299 - ETA: 13s - loss: 2.9821 - accuracy: 0.300 - ETA: 10s - loss: 2.9856 - accuracy: 0.298 - ETA: 8s - loss: 2.9887 - accuracy: 0.298 - ETA: 6s - loss: 2.9864 - accuracy: 0.29 - ETA: 4s - loss: 2.9747 - accuracy: 0.30 - ETA: 2s - loss: 2.9822 - accuracy: 0.30 - ETA: 0s - loss: 2.9825 - accuracy: 0.30 - 41s 5ms/step - loss: 2.9815 - accuracy: 0.3004 - val_loss: 3.6321 - val_accuracy: 0.1400\n",
      "Epoch 26/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.9919 - accuracy: 0.289 - ETA: 32s - loss: 3.0041 - accuracy: 0.286 - ETA: 30s - loss: 2.9751 - accuracy: 0.296 - ETA: 28s - loss: 2.9622 - accuracy: 0.303 - ETA: 27s - loss: 2.9789 - accuracy: 0.297 - ETA: 25s - loss: 2.9739 - accuracy: 0.303 - ETA: 22s - loss: 2.9510 - accuracy: 0.309 - ETA: 20s - loss: 2.9470 - accuracy: 0.309 - ETA: 18s - loss: 2.9444 - accuracy: 0.309 - ETA: 15s - loss: 2.9403 - accuracy: 0.310 - ETA: 13s - loss: 2.9386 - accuracy: 0.311 - ETA: 11s - loss: 2.9371 - accuracy: 0.312 - ETA: 9s - loss: 2.9345 - accuracy: 0.312 - ETA: 6s - loss: 2.9343 - accuracy: 0.31 - ETA: 4s - loss: 2.9328 - accuracy: 0.31 - ETA: 2s - loss: 2.9329 - accuracy: 0.30 - ETA: 0s - loss: 2.9322 - accuracy: 0.30 - 40s 5ms/step - loss: 2.9310 - accuracy: 0.3096 - val_loss: 3.6445 - val_accuracy: 0.1500\n",
      "Epoch 27/100\n",
      "8757/8757 [==============================] - ETA: 33s - loss: 2.9632 - accuracy: 0.302 - ETA: 32s - loss: 2.9439 - accuracy: 0.310 - ETA: 30s - loss: 2.9442 - accuracy: 0.308 - ETA: 28s - loss: 2.9392 - accuracy: 0.309 - ETA: 26s - loss: 2.9558 - accuracy: 0.303 - ETA: 23s - loss: 2.9499 - accuracy: 0.303 - ETA: 21s - loss: 2.9244 - accuracy: 0.309 - ETA: 19s - loss: 2.9147 - accuracy: 0.309 - ETA: 17s - loss: 2.9070 - accuracy: 0.311 - ETA: 15s - loss: 2.9042 - accuracy: 0.313 - ETA: 13s - loss: 2.8935 - accuracy: 0.316 - ETA: 10s - loss: 2.8938 - accuracy: 0.315 - ETA: 8s - loss: 2.8915 - accuracy: 0.315 - ETA: 6s - loss: 2.8792 - accuracy: 0.31 - ETA: 4s - loss: 2.8821 - accuracy: 0.31 - ETA: 2s - loss: 2.8833 - accuracy: 0.31 - ETA: 0s - loss: 2.8844 - accuracy: 0.31 - 40s 5ms/step - loss: 2.8846 - accuracy: 0.3167 - val_loss: 3.6540 - val_accuracy: 0.1500\n",
      "Epoch 28/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.9686 - accuracy: 0.318 - ETA: 33s - loss: 2.9488 - accuracy: 0.316 - ETA: 30s - loss: 2.8959 - accuracy: 0.328 - ETA: 28s - loss: 2.8735 - accuracy: 0.332 - ETA: 26s - loss: 2.8581 - accuracy: 0.336 - ETA: 24s - loss: 2.8623 - accuracy: 0.329 - ETA: 21s - loss: 2.8547 - accuracy: 0.329 - ETA: 19s - loss: 2.8473 - accuracy: 0.330 - ETA: 17s - loss: 2.8489 - accuracy: 0.329 - ETA: 15s - loss: 2.8367 - accuracy: 0.331 - ETA: 13s - loss: 2.8361 - accuracy: 0.331 - ETA: 11s - loss: 2.8416 - accuracy: 0.328 - ETA: 8s - loss: 2.8405 - accuracy: 0.328 - ETA: 6s - loss: 2.8349 - accuracy: 0.32 - ETA: 4s - loss: 2.8317 - accuracy: 0.32 - ETA: 2s - loss: 2.8314 - accuracy: 0.32 - ETA: 0s - loss: 2.8311 - accuracy: 0.32 - 40s 5ms/step - loss: 2.8304 - accuracy: 0.3282 - val_loss: 3.7167 - val_accuracy: 0.1500\n",
      "Epoch 29/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.8240 - accuracy: 0.326 - ETA: 32s - loss: 2.8428 - accuracy: 0.334 - ETA: 30s - loss: 2.8501 - accuracy: 0.324 - ETA: 27s - loss: 2.8283 - accuracy: 0.323 - ETA: 25s - loss: 2.8342 - accuracy: 0.325 - ETA: 23s - loss: 2.8095 - accuracy: 0.329 - ETA: 21s - loss: 2.8091 - accuracy: 0.329 - ETA: 19s - loss: 2.8126 - accuracy: 0.329 - ETA: 17s - loss: 2.8227 - accuracy: 0.326 - ETA: 15s - loss: 2.8294 - accuracy: 0.323 - ETA: 13s - loss: 2.8294 - accuracy: 0.322 - ETA: 11s - loss: 2.8225 - accuracy: 0.326 - ETA: 8s - loss: 2.8156 - accuracy: 0.327 - ETA: 6s - loss: 2.8037 - accuracy: 0.32 - ETA: 4s - loss: 2.8064 - accuracy: 0.32 - ETA: 2s - loss: 2.7992 - accuracy: 0.33 - ETA: 0s - loss: 2.7994 - accuracy: 0.33 - 40s 5ms/step - loss: 2.7991 - accuracy: 0.3305 - val_loss: 3.7138 - val_accuracy: 0.1500\n",
      "Epoch 30/100\n",
      "8757/8757 [==============================] - ETA: 36s - loss: 2.6650 - accuracy: 0.367 - ETA: 35s - loss: 2.7169 - accuracy: 0.345 - ETA: 32s - loss: 2.7174 - accuracy: 0.350 - ETA: 30s - loss: 2.7258 - accuracy: 0.346 - ETA: 28s - loss: 2.7274 - accuracy: 0.343 - ETA: 25s - loss: 2.7470 - accuracy: 0.336 - ETA: 23s - loss: 2.7392 - accuracy: 0.336 - ETA: 20s - loss: 2.7449 - accuracy: 0.336 - ETA: 18s - loss: 2.7559 - accuracy: 0.331 - ETA: 16s - loss: 2.7487 - accuracy: 0.329 - ETA: 13s - loss: 2.7414 - accuracy: 0.333 - ETA: 11s - loss: 2.7316 - accuracy: 0.335 - ETA: 9s - loss: 2.7316 - accuracy: 0.338 - ETA: 7s - loss: 2.7373 - accuracy: 0.33 - ETA: 4s - loss: 2.7315 - accuracy: 0.34 - ETA: 2s - loss: 2.7302 - accuracy: 0.34 - ETA: 0s - loss: 2.7363 - accuracy: 0.34 - 42s 5ms/step - loss: 2.7375 - accuracy: 0.3419 - val_loss: 3.7715 - val_accuracy: 0.1400\n",
      "Epoch 31/100\n",
      "8757/8757 [==============================] - ETA: 36s - loss: 2.7432 - accuracy: 0.332 - ETA: 33s - loss: 2.6931 - accuracy: 0.348 - ETA: 31s - loss: 2.6466 - accuracy: 0.362 - ETA: 29s - loss: 2.6445 - accuracy: 0.365 - ETA: 26s - loss: 2.6558 - accuracy: 0.362 - ETA: 24s - loss: 2.6381 - accuracy: 0.366 - ETA: 22s - loss: 2.6439 - accuracy: 0.365 - ETA: 20s - loss: 2.6497 - accuracy: 0.362 - ETA: 18s - loss: 2.6619 - accuracy: 0.358 - ETA: 15s - loss: 2.6670 - accuracy: 0.357 - ETA: 13s - loss: 2.6663 - accuracy: 0.358 - ETA: 11s - loss: 2.6660 - accuracy: 0.358 - ETA: 9s - loss: 2.6704 - accuracy: 0.357 - ETA: 6s - loss: 2.6751 - accuracy: 0.35 - ETA: 4s - loss: 2.6779 - accuracy: 0.35 - ETA: 2s - loss: 2.6818 - accuracy: 0.35 - ETA: 0s - loss: 2.6814 - accuracy: 0.35 - 42s 5ms/step - loss: 2.6793 - accuracy: 0.3556 - val_loss: 3.8042 - val_accuracy: 0.1300\n",
      "Epoch 32/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.5353 - accuracy: 0.400 - ETA: 34s - loss: 2.6531 - accuracy: 0.361 - ETA: 31s - loss: 2.6646 - accuracy: 0.367 - ETA: 29s - loss: 2.6588 - accuracy: 0.364 - ETA: 27s - loss: 2.6570 - accuracy: 0.362 - ETA: 25s - loss: 2.6414 - accuracy: 0.365 - ETA: 22s - loss: 2.6578 - accuracy: 0.360 - ETA: 20s - loss: 2.6530 - accuracy: 0.359 - ETA: 18s - loss: 2.6486 - accuracy: 0.361 - ETA: 16s - loss: 2.6471 - accuracy: 0.361 - ETA: 13s - loss: 2.6250 - accuracy: 0.367 - ETA: 11s - loss: 2.6376 - accuracy: 0.365 - ETA: 9s - loss: 2.6299 - accuracy: 0.365 - ETA: 7s - loss: 2.6255 - accuracy: 0.36 - ETA: 4s - loss: 2.6271 - accuracy: 0.36 - ETA: 2s - loss: 2.6272 - accuracy: 0.36 - ETA: 0s - loss: 2.6226 - accuracy: 0.36 - 41s 5ms/step - loss: 2.6232 - accuracy: 0.3671 - val_loss: 3.8849 - val_accuracy: 0.1100\n",
      "Epoch 33/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.6647 - accuracy: 0.361 - ETA: 32s - loss: 2.6159 - accuracy: 0.364 - ETA: 30s - loss: 2.6353 - accuracy: 0.365 - ETA: 28s - loss: 2.6137 - accuracy: 0.371 - ETA: 26s - loss: 2.6054 - accuracy: 0.372 - ETA: 24s - loss: 2.6211 - accuracy: 0.367 - ETA: 21s - loss: 2.6228 - accuracy: 0.365 - ETA: 19s - loss: 2.6124 - accuracy: 0.369 - ETA: 17s - loss: 2.6119 - accuracy: 0.368 - ETA: 15s - loss: 2.5960 - accuracy: 0.374 - ETA: 13s - loss: 2.5924 - accuracy: 0.375 - ETA: 10s - loss: 2.5851 - accuracy: 0.376 - ETA: 8s - loss: 2.5748 - accuracy: 0.377 - ETA: 6s - loss: 2.5743 - accuracy: 0.37 - ETA: 4s - loss: 2.5722 - accuracy: 0.37 - ETA: 2s - loss: 2.5777 - accuracy: 0.37 - ETA: 0s - loss: 2.5763 - accuracy: 0.37 - 39s 4ms/step - loss: 2.5771 - accuracy: 0.3751 - val_loss: 3.9227 - val_accuracy: 0.1300\n",
      "Epoch 34/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 2.5297 - accuracy: 0.388 - ETA: 32s - loss: 2.5121 - accuracy: 0.394 - ETA: 30s - loss: 2.5217 - accuracy: 0.391 - ETA: 28s - loss: 2.5175 - accuracy: 0.394 - ETA: 25s - loss: 2.5200 - accuracy: 0.389 - ETA: 23s - loss: 2.5159 - accuracy: 0.393 - ETA: 21s - loss: 2.5119 - accuracy: 0.391 - ETA: 19s - loss: 2.5095 - accuracy: 0.391 - ETA: 17s - loss: 2.5072 - accuracy: 0.392 - ETA: 15s - loss: 2.5159 - accuracy: 0.391 - ETA: 12s - loss: 2.5095 - accuracy: 0.394 - ETA: 10s - loss: 2.5111 - accuracy: 0.393 - ETA: 8s - loss: 2.5232 - accuracy: 0.389 - ETA: 6s - loss: 2.5314 - accuracy: 0.38 - ETA: 4s - loss: 2.5249 - accuracy: 0.38 - ETA: 2s - loss: 2.5211 - accuracy: 0.38 - ETA: 0s - loss: 2.5255 - accuracy: 0.38 - 39s 4ms/step - loss: 2.5258 - accuracy: 0.3894 - val_loss: 3.9504 - val_accuracy: 0.1200\n",
      "Epoch 35/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.4922 - accuracy: 0.419 - ETA: 32s - loss: 2.5372 - accuracy: 0.396 - ETA: 30s - loss: 2.5123 - accuracy: 0.402 - ETA: 27s - loss: 2.5148 - accuracy: 0.399 - ETA: 25s - loss: 2.4922 - accuracy: 0.407 - ETA: 23s - loss: 2.4733 - accuracy: 0.408 - ETA: 21s - loss: 2.4827 - accuracy: 0.405 - ETA: 19s - loss: 2.4691 - accuracy: 0.406 - ETA: 17s - loss: 2.4647 - accuracy: 0.408 - ETA: 15s - loss: 2.4579 - accuracy: 0.411 - ETA: 13s - loss: 2.4574 - accuracy: 0.410 - ETA: 10s - loss: 2.4612 - accuracy: 0.407 - ETA: 8s - loss: 2.4548 - accuracy: 0.406 - ETA: 6s - loss: 2.4593 - accuracy: 0.40 - ETA: 4s - loss: 2.4651 - accuracy: 0.40 - ETA: 2s - loss: 2.4674 - accuracy: 0.40 - ETA: 0s - loss: 2.4722 - accuracy: 0.40 - 39s 4ms/step - loss: 2.4720 - accuracy: 0.4014 - val_loss: 3.9825 - val_accuracy: 0.1200\n",
      "Epoch 36/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.4504 - accuracy: 0.384 - ETA: 32s - loss: 2.4442 - accuracy: 0.389 - ETA: 30s - loss: 2.4396 - accuracy: 0.392 - ETA: 28s - loss: 2.4531 - accuracy: 0.387 - ETA: 25s - loss: 2.4317 - accuracy: 0.396 - ETA: 23s - loss: 2.4197 - accuracy: 0.401 - ETA: 21s - loss: 2.3983 - accuracy: 0.413 - ETA: 19s - loss: 2.4057 - accuracy: 0.417 - ETA: 17s - loss: 2.4002 - accuracy: 0.420 - ETA: 15s - loss: 2.4042 - accuracy: 0.420 - ETA: 12s - loss: 2.4073 - accuracy: 0.418 - ETA: 10s - loss: 2.4023 - accuracy: 0.419 - ETA: 8s - loss: 2.4106 - accuracy: 0.417 - ETA: 6s - loss: 2.4121 - accuracy: 0.41 - ETA: 4s - loss: 2.4149 - accuracy: 0.41 - ETA: 2s - loss: 2.4094 - accuracy: 0.41 - ETA: 0s - loss: 2.4124 - accuracy: 0.41 - 39s 4ms/step - loss: 2.4140 - accuracy: 0.4166 - val_loss: 4.0072 - val_accuracy: 0.1000\n",
      "Epoch 37/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.3766 - accuracy: 0.433 - ETA: 31s - loss: 2.4158 - accuracy: 0.418 - ETA: 29s - loss: 2.4003 - accuracy: 0.423 - ETA: 27s - loss: 2.3969 - accuracy: 0.422 - ETA: 25s - loss: 2.3591 - accuracy: 0.430 - ETA: 23s - loss: 2.3663 - accuracy: 0.427 - ETA: 21s - loss: 2.3693 - accuracy: 0.424 - ETA: 19s - loss: 2.3669 - accuracy: 0.427 - ETA: 17s - loss: 2.3580 - accuracy: 0.429 - ETA: 15s - loss: 2.3617 - accuracy: 0.427 - ETA: 12s - loss: 2.3636 - accuracy: 0.428 - ETA: 10s - loss: 2.3725 - accuracy: 0.423 - ETA: 8s - loss: 2.3724 - accuracy: 0.425 - ETA: 6s - loss: 2.3761 - accuracy: 0.42 - ETA: 4s - loss: 2.3717 - accuracy: 0.42 - ETA: 2s - loss: 2.3614 - accuracy: 0.42 - ETA: 0s - loss: 2.3538 - accuracy: 0.43 - 39s 4ms/step - loss: 2.3542 - accuracy: 0.4298 - val_loss: 4.0936 - val_accuracy: 0.1000\n",
      "Epoch 38/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.1613 - accuracy: 0.472 - ETA: 32s - loss: 2.2335 - accuracy: 0.446 - ETA: 30s - loss: 2.2534 - accuracy: 0.449 - ETA: 28s - loss: 2.2883 - accuracy: 0.449 - ETA: 26s - loss: 2.2973 - accuracy: 0.447 - ETA: 23s - loss: 2.3069 - accuracy: 0.445 - ETA: 21s - loss: 2.3128 - accuracy: 0.442 - ETA: 19s - loss: 2.3186 - accuracy: 0.441 - ETA: 17s - loss: 2.3186 - accuracy: 0.442 - ETA: 15s - loss: 2.3255 - accuracy: 0.440 - ETA: 13s - loss: 2.3205 - accuracy: 0.442 - ETA: 10s - loss: 2.3058 - accuracy: 0.445 - ETA: 8s - loss: 2.3050 - accuracy: 0.445 - ETA: 6s - loss: 2.2999 - accuracy: 0.44 - ETA: 4s - loss: 2.2966 - accuracy: 0.44 - ETA: 2s - loss: 2.2927 - accuracy: 0.44 - ETA: 0s - loss: 2.2935 - accuracy: 0.44 - 40s 5ms/step - loss: 2.2944 - accuracy: 0.4460 - val_loss: 4.1333 - val_accuracy: 0.1000\n",
      "Epoch 39/100\n",
      "8757/8757 [==============================] - ETA: 33s - loss: 2.3367 - accuracy: 0.410 - ETA: 31s - loss: 2.3015 - accuracy: 0.427 - ETA: 29s - loss: 2.2871 - accuracy: 0.438 - ETA: 27s - loss: 2.3022 - accuracy: 0.431 - ETA: 25s - loss: 2.3048 - accuracy: 0.436 - ETA: 23s - loss: 2.2835 - accuracy: 0.445 - ETA: 21s - loss: 2.2677 - accuracy: 0.452 - ETA: 19s - loss: 2.2615 - accuracy: 0.455 - ETA: 17s - loss: 2.2613 - accuracy: 0.455 - ETA: 15s - loss: 2.2576 - accuracy: 0.457 - ETA: 12s - loss: 2.2456 - accuracy: 0.459 - ETA: 10s - loss: 2.2353 - accuracy: 0.461 - ETA: 8s - loss: 2.2467 - accuracy: 0.458 - ETA: 6s - loss: 2.2515 - accuracy: 0.45 - ETA: 4s - loss: 2.2469 - accuracy: 0.45 - ETA: 2s - loss: 2.2443 - accuracy: 0.45 - ETA: 0s - loss: 2.2429 - accuracy: 0.46 - 39s 4ms/step - loss: 2.2421 - accuracy: 0.4604 - val_loss: 4.1521 - val_accuracy: 0.1100\n",
      "Epoch 40/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.2663 - accuracy: 0.443 - ETA: 33s - loss: 2.2441 - accuracy: 0.447 - ETA: 31s - loss: 2.2108 - accuracy: 0.461 - ETA: 28s - loss: 2.2148 - accuracy: 0.463 - ETA: 26s - loss: 2.2332 - accuracy: 0.464 - ETA: 24s - loss: 2.2229 - accuracy: 0.470 - ETA: 21s - loss: 2.2070 - accuracy: 0.473 - ETA: 19s - loss: 2.2102 - accuracy: 0.471 - ETA: 17s - loss: 2.2185 - accuracy: 0.468 - ETA: 15s - loss: 2.2181 - accuracy: 0.469 - ETA: 13s - loss: 2.2207 - accuracy: 0.471 - ETA: 10s - loss: 2.2208 - accuracy: 0.471 - ETA: 8s - loss: 2.2264 - accuracy: 0.469 - ETA: 6s - loss: 2.2290 - accuracy: 0.46 - ETA: 4s - loss: 2.2305 - accuracy: 0.46 - ETA: 2s - loss: 2.2296 - accuracy: 0.46 - ETA: 0s - loss: 2.2259 - accuracy: 0.46 - 39s 4ms/step - loss: 2.2269 - accuracy: 0.4682 - val_loss: 4.2203 - val_accuracy: 0.0900\n",
      "Epoch 41/100\n",
      "8757/8757 [==============================] - ETA: 33s - loss: 2.1765 - accuracy: 0.470 - ETA: 31s - loss: 2.1146 - accuracy: 0.506 - ETA: 29s - loss: 2.1598 - accuracy: 0.501 - ETA: 27s - loss: 2.1476 - accuracy: 0.500 - ETA: 25s - loss: 2.1511 - accuracy: 0.491 - ETA: 23s - loss: 2.1575 - accuracy: 0.486 - ETA: 21s - loss: 2.1547 - accuracy: 0.489 - ETA: 19s - loss: 2.1799 - accuracy: 0.483 - ETA: 17s - loss: 2.1753 - accuracy: 0.484 - ETA: 15s - loss: 2.1721 - accuracy: 0.483 - ETA: 12s - loss: 2.1653 - accuracy: 0.484 - ETA: 10s - loss: 2.1720 - accuracy: 0.482 - ETA: 8s - loss: 2.1714 - accuracy: 0.483 - ETA: 6s - loss: 2.1766 - accuracy: 0.48 - ETA: 4s - loss: 2.1696 - accuracy: 0.48 - ETA: 2s - loss: 2.1721 - accuracy: 0.48 - ETA: 0s - loss: 2.1748 - accuracy: 0.48 - 39s 4ms/step - loss: 2.1742 - accuracy: 0.4826 - val_loss: 4.2861 - val_accuracy: 0.1000\n",
      "Epoch 42/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.1093 - accuracy: 0.482 - ETA: 32s - loss: 2.1514 - accuracy: 0.476 - ETA: 30s - loss: 2.1508 - accuracy: 0.479 - ETA: 27s - loss: 2.1476 - accuracy: 0.483 - ETA: 25s - loss: 2.1724 - accuracy: 0.478 - ETA: 23s - loss: 2.1534 - accuracy: 0.484 - ETA: 21s - loss: 2.1445 - accuracy: 0.491 - ETA: 19s - loss: 2.1397 - accuracy: 0.490 - ETA: 17s - loss: 2.1348 - accuracy: 0.489 - ETA: 15s - loss: 2.1144 - accuracy: 0.494 - ETA: 12s - loss: 2.1153 - accuracy: 0.495 - ETA: 10s - loss: 2.1114 - accuracy: 0.496 - ETA: 8s - loss: 2.1106 - accuracy: 0.496 - ETA: 6s - loss: 2.1128 - accuracy: 0.49 - ETA: 4s - loss: 2.1077 - accuracy: 0.49 - ETA: 2s - loss: 2.1080 - accuracy: 0.49 - ETA: 0s - loss: 2.1086 - accuracy: 0.49 - 39s 4ms/step - loss: 2.1100 - accuracy: 0.4981 - val_loss: 4.3249 - val_accuracy: 0.1000\n",
      "Epoch 43/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 2.0113 - accuracy: 0.535 - ETA: 32s - loss: 2.0741 - accuracy: 0.512 - ETA: 30s - loss: 2.0709 - accuracy: 0.505 - ETA: 28s - loss: 2.0726 - accuracy: 0.504 - ETA: 25s - loss: 2.0619 - accuracy: 0.507 - ETA: 23s - loss: 2.0786 - accuracy: 0.501 - ETA: 21s - loss: 2.0694 - accuracy: 0.505 - ETA: 19s - loss: 2.0711 - accuracy: 0.504 - ETA: 17s - loss: 2.0687 - accuracy: 0.506 - ETA: 15s - loss: 2.0682 - accuracy: 0.509 - ETA: 12s - loss: 2.0605 - accuracy: 0.513 - ETA: 10s - loss: 2.0622 - accuracy: 0.512 - ETA: 8s - loss: 2.0660 - accuracy: 0.513 - ETA: 6s - loss: 2.0622 - accuracy: 0.51 - ETA: 4s - loss: 2.0600 - accuracy: 0.51 - ETA: 2s - loss: 2.0551 - accuracy: 0.51 - ETA: 0s - loss: 2.0589 - accuracy: 0.51 - 39s 4ms/step - loss: 2.0592 - accuracy: 0.5140 - val_loss: 4.3638 - val_accuracy: 0.1100\n",
      "Epoch 44/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 1.9232 - accuracy: 0.539 - ETA: 32s - loss: 1.9414 - accuracy: 0.536 - ETA: 30s - loss: 1.9801 - accuracy: 0.533 - ETA: 28s - loss: 1.9763 - accuracy: 0.539 - ETA: 25s - loss: 2.0007 - accuracy: 0.534 - ETA: 23s - loss: 2.0205 - accuracy: 0.528 - ETA: 21s - loss: 2.0181 - accuracy: 0.525 - ETA: 19s - loss: 2.0168 - accuracy: 0.524 - ETA: 17s - loss: 2.0179 - accuracy: 0.524 - ETA: 15s - loss: 2.0039 - accuracy: 0.526 - ETA: 13s - loss: 2.0087 - accuracy: 0.523 - ETA: 10s - loss: 2.0146 - accuracy: 0.521 - ETA: 8s - loss: 2.0069 - accuracy: 0.523 - ETA: 6s - loss: 2.0083 - accuracy: 0.52 - ETA: 4s - loss: 2.0070 - accuracy: 0.52 - ETA: 2s - loss: 2.0093 - accuracy: 0.52 - ETA: 0s - loss: 2.0073 - accuracy: 0.52 - 39s 4ms/step - loss: 2.0058 - accuracy: 0.5275 - val_loss: 4.3867 - val_accuracy: 0.1100\n",
      "Epoch 45/100\n",
      "8757/8757 [==============================] - ETA: 34s - loss: 1.9957 - accuracy: 0.509 - ETA: 32s - loss: 2.0072 - accuracy: 0.514 - ETA: 30s - loss: 2.0080 - accuracy: 0.513 - ETA: 27s - loss: 2.0025 - accuracy: 0.520 - ETA: 25s - loss: 2.0193 - accuracy: 0.521 - ETA: 23s - loss: 2.0060 - accuracy: 0.526 - ETA: 21s - loss: 2.0225 - accuracy: 0.516 - ETA: 19s - loss: 2.0275 - accuracy: 0.517 - ETA: 17s - loss: 2.0199 - accuracy: 0.518 - ETA: 15s - loss: 2.0231 - accuracy: 0.517 - ETA: 13s - loss: 2.0294 - accuracy: 0.517 - ETA: 10s - loss: 2.0243 - accuracy: 0.521 - ETA: 8s - loss: 2.0235 - accuracy: 0.521 - ETA: 6s - loss: 2.0139 - accuracy: 0.52 - ETA: 4s - loss: 2.0235 - accuracy: 0.52 - ETA: 2s - loss: 2.0244 - accuracy: 0.51 - ETA: 0s - loss: 2.0194 - accuracy: 0.52 - 40s 5ms/step - loss: 2.0188 - accuracy: 0.5211 - val_loss: 4.4656 - val_accuracy: 0.1000\n",
      "Epoch 46/100\n",
      "8757/8757 [==============================] - ETA: 35s - loss: 1.9200 - accuracy: 0.544 - ETA: 32s - loss: 1.9502 - accuracy: 0.543 - ETA: 31s - loss: 1.9863 - accuracy: 0.536 - ETA: 28s - loss: 1.9822 - accuracy: 0.535 - ETA: 26s - loss: 1.9649 - accuracy: 0.535 - ETA: 23s - loss: 1.9656 - accuracy: 0.533 - ETA: 21s - loss: 1.9589 - accuracy: 0.534 - ETA: 19s - loss: 1.9468 - accuracy: 0.538 - ETA: 17s - loss: 1.9560 - accuracy: 0.536 - ETA: 15s - loss: 1.9549 - accuracy: 0.538 - ETA: 13s - loss: 1.9520 - accuracy: 0.538 - ETA: 11s - loss: 1.9507 - accuracy: 0.540 - ETA: 9s - loss: 1.9426 - accuracy: 0.543 - ETA: 6s - loss: 1.9443 - accuracy: 0.54 - ETA: 4s - loss: 1.9386 - accuracy: 0.54 - ETA: 2s - loss: 1.9347 - accuracy: 0.54 - ETA: 0s - loss: 1.9405 - accuracy: 0.54 - 41s 5ms/step - loss: 1.9395 - accuracy: 0.5429 - val_loss: 4.4243 - val_accuracy: 0.1000\n",
      "Epoch 47/100\n",
      "4608/8757 [==============>...............] - ETA: 36s - loss: 1.8989 - accuracy: 0.541 - ETA: 33s - loss: 1.8383 - accuracy: 0.563 - ETA: 31s - loss: 1.8602 - accuracy: 0.557 - ETA: 29s - loss: 1.8647 - accuracy: 0.555 - ETA: 27s - loss: 1.8840 - accuracy: 0.556 - ETA: 25s - loss: 1.8615 - accuracy: 0.560 - ETA: 22s - loss: 1.8791 - accuracy: 0.555 - ETA: 20s - loss: 1.8885 - accuracy: 0.552 - ETA: 18s - loss: 1.8870 - accuracy: 0.5543"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-859db7371b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m simple_lstm.fit(data_train_sequences[:-100], target_train_ohe[:-100], \n\u001b[0;32m      2\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                    validation_data=(data_train_sequences[-100:], target_train_ohe[-100:]) )\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_lstm.fit(data_train_sequences[:-100], target_train_ohe[:-100], \n",
    "                   epochs=100, batch_size=512, verbose=1, \n",
    "                   validation_data=(data_train_sequences[-100:], target_train_ohe[-100:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sim_conv_model.history.history['val_accuracy'])\n",
    "plt.plot(sim_conv_model.history.history['accuracy'])\n",
    "plt.title('val_accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sim_conv_model.history.history['val_loss'])\n",
    "plt.plot(sim_conv_model.history.history['loss'])\n",
    "plt.title('val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
